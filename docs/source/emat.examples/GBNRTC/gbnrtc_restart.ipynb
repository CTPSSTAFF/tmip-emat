{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restarting core files models after a core model run error\n",
    "\n",
    "The simplest way to setup and run experiments, and then to import the results and archive the model is to use `run_experiments`. However, if the core files model fails during an experiment and it is preferable to not restart that particular experiment from scratch but to finish the run manually, the individual functions of the function may be called before running the remaining experiments.\n",
    "\n",
    "This example first shows the process to run a set of designed experiments through the core model, then shows the steps to run the individual functions of `run_experiments`. Finally, the command to run the remaining functions is shown using the pending_only argument of run_experiments.  \n",
    "\n",
    "This example assumes that an experiment design has been created, for examples on creating an \n",
    "experiment design, see the examples in the `design_experiments`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emat\n",
    "emat.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, numpy, os\n",
    "from emat.util.loggers import log_to_stderr\n",
    "log = log_to_stderr(level=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to database of experiments\n",
    "db = emat.SQLiteDB(\"emat_db/gbnrtc.db\",initialize=False)\n",
    "scope = db.read_scope('GBNRTC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model object\n",
    "from emat.model import GBNRTCModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = GBNRTCModel(\n",
    "    configuration='gbnrtc_model_config.yaml',\n",
    "    scope=scope,\n",
    "    db=db,\n",
    ")\n",
    "g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run All Experiments\n",
    "\n",
    "If all goes well and the core model will be run on a single system, the following function is the only one necessary to run to execute the core model across the set of experiments and populate the database with the model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_df = g.read_experiments(design_name='lhs')\n",
    "exp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.run_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restart single experiment after manual completion of run\n",
    "\n",
    "Now let's assume that during the first experiment (ID 1) the model crashed on the last speed-feedback iteration. Rather than restart the model run at the beginning (as run_experiments would do) the user may complete the model run manually and then post process and import the model outputs using TMIP-EMAT\n",
    "\n",
    "If the user prefers to simply restart the core model from the beginning, skip this step and proceed to the following section on Running Pending Experiments.\n",
    "\n",
    "The first step is to identify the experiment that failed. This should be displayed in the error message from the command above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's assume that the model failed on the 1st experiment, \n",
    "# note this matches the experiment column in the data frame above\n",
    "exp_id = 1\n",
    "\n",
    "# get the measure names from the scope for reference\n",
    "measure_names = g.scope.get_measure_names()\n",
    "\n",
    "# get a dictionary of the parameters for reference\n",
    "params = exp_df.loc[exp_id]\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, call the post processor to generate the output metrics in a format \n",
    "# that tmip-emat can read\n",
    "\n",
    "g.post_process(params, measure_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, import the metrics and write them to the database\n",
    "measures_dictionary = g.load_measures(measure_names)\n",
    "m_df = pandas.DataFrame(measures_dictionary, index=[exp_id])\n",
    "\n",
    "# Assign to outcomes_output, for ema_workbench compatibility\n",
    "g.outcomes_output = measures_dictionary\n",
    "g.db.write_experiment_measures(g.scope.name, g.metamodel_id, m_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm that the metrics were imported into the database\n",
    "g.read_experiments(design_name='lhs').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, archive the model\n",
    "archive_path = g.get_experiment_archive_path(exp_id)\n",
    "g.archive(params, archive_path, exp_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Pending Experiments\n",
    "\n",
    "Now that the partial run has been wrapped up, imported, and archived, we can resume running the remaining experiments, with fingers crossed!\n",
    "\n",
    "To identify the experiments that haven't been run, we can identify the pending experiments and pass those to the run_experiments method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the pending experiments, note that experiment 1 is not included\n",
    "pending_exp = g.read_experiments(design_name='lhs',only_pending=True)\n",
    "pending_exp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.run_experiments(design=pending_exp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
